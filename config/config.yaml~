runtime: {cycles: 2, dry_run: true, seed: 42}
engine: llama.cpp
llm: {gen_defaults: {temp: 0.6, top_p: 0.95, repeat_penalty: 1.1, max_tokens: 256, ctx: 4096}}
models:
  neutral_a: /home/sentinel/.var/app/io.gpt4all.gpt4all/data/nomic.ai/GPT4All/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf
  mooded_b:  /home/sentinel/.var/app/io.gpt4all.gpt4all/data/nomic.ai/GPT4All/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf
  summarizer: /home/sentinel/.var/app/io.gpt4all.gpt4all/data/nomic.ai/GPT4All/mistral-7b-instruct-v0.1.Q4_0.gguf
  coder:      /home/sentinel/.var/app/io.gpt4all.gpt4all/data/nomic.ai/GPT4All/qwen2.5-coder-7b-instruct-q4_0.gguf
persona: {current: Analytical, dir: ./personas}
prompts: {dir: ./prompts, files: {}}
memory: {faiss_path: ./data/vectors.faiss, sqlite_path: ./data/meta.sqlite3, recall_k: 8, alpha: 0.7}
safety: {allowlist_tools: [], veto_risk: 0.6}

